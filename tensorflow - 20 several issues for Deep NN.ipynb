{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 보다 ReLU(Rectified Linear Unit)가 더 좋아"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN에서는 sigmoid보다는 ReLU 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU = max(0,x)\n",
    "#### Leaky ReLU = max(0.1x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sigmoid를 이용하여  hidden layer를 9개 까지 쌓았는데 오히려 결과가 더 안 좋다?\n",
    "> vanishing gradient problem\n",
    "> - sigmoid 함수는 0~1사이의 값, hidden layer가 많아질 수록 초기 input값이 미치는 영향력은 줄어듬. 하지만 초기 input값은 중요한 의미를 가짐\n",
    "> - dcost/dw = 0 이라는 것은 w(초기 input에 대한 가중치)가 cost에 미치는 영향이 거의 없다는 의미이고, cost에 영향이 없다는 것은 초기 input값이 출력 y에 미치는 영향이 거의 없다는 의미임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer에서는 sigmoid를 사용(출력값이 0~1사이 값 가지도록)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight 초기화를 잘해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 초기 Weight를 어떤 식으로 주느냐에 따라 cost ftn이 빠르게 수렴\n",
    "> - w 초기값을 모두 0을 주게 될 경우 cost 줄어들지 않음\n",
    "> - RBM(restricted boatmann machine) : Fine Tuning => 지금은 잘 안 씀\n",
    "> - Xavier initialization : 입력 개수와 출력 개수에 따라 weight 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout과 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep NN overfitting?\n",
    "> - 학습 데이터에 대한 accuracy는 높은데 테스트 데이터에 대한 accuracy는 낮음\n",
    "> - hidden layer가 많아지면 많아질수록 overfitting의 위험성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solutions for overfitting\n",
    "> - More training data\n",
    "> - Reduce the number of features\n",
    "> - Regularization : 너무 W를 크게 주지 말자. W가 너무 치우칠 경우 overfitting<br>\n",
    "> l2reg = 0.001*tf.reduce_sum(tf.square(W))\n",
    "> - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout : A Simple way to prevent NN from overfitting\n",
    "> - random하게 뉴런을 kill 해버림. 학습을 반복할 때 random으로 뉴런을 쉬게 함\n",
    "> - 학습할 때만 dropout하고, model 생성 후 실전 evaluation에서는 dropout하면 안 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble(앙상블)?\n",
    "> - 여러 가지 model을 조합하는 것\n",
    "> - 2%~5% 정도 성능이 향상됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
