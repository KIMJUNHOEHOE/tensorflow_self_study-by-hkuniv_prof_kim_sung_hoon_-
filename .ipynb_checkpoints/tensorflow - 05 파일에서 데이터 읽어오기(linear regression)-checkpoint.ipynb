{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 73.,  66.,  70., 142.],\n",
       "       [ 53.,  46.,  55., 101.],\n",
       "       [ 69.,  74.,  77., 149.],\n",
       "       [ 47.,  56.,  60., 115.],\n",
       "       [ 87.,  79.,  90., 175.],\n",
       "       [ 79.,  70.,  88., 164.],\n",
       "       [ 69.,  70.,  73., 141.],\n",
       "       [ 70.,  65.,  74., 141.],\n",
       "       [ 93.,  95.,  91., 184.],\n",
       "       [ 79.,  80.,  73., 152.],\n",
       "       [ 70.,  73.,  78., 148.],\n",
       "       [ 93.,  89.,  96., 192.],\n",
       "       [ 78.,  75.,  68., 147.],\n",
       "       [ 81.,  90.,  93., 183.],\n",
       "       [ 88.,  92.,  86., 177.],\n",
       "       [ 78.,  83.,  77., 159.],\n",
       "       [ 82.,  86.,  90., 177.],\n",
       "       [ 86.,  82.,  89., 175.],\n",
       "       [ 78.,  83.,  85., 175.],\n",
       "       [ 76.,  83.,  71., 149.],\n",
       "       [ 96.,  93.,  95., 192.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.loadtxt('./data/data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:,:3]\n",
    "y_data = xy[:,[-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) \n",
      " [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] 25\n",
      "(25, 1) \n",
      " [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]] 25\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape, '\\n', x_data, len(x_data)) # data 확인\n",
    "print(y_data.shape, '\\n', y_data, len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  791.1328 \n",
      "Prediction:\n",
      " [[185.34   ]\n",
      " [211.09311]\n",
      " [213.91539]\n",
      " [233.18855]\n",
      " [158.07416]\n",
      " [115.83804]\n",
      " [178.3998 ]\n",
      " [137.90709]\n",
      " [195.01056]\n",
      " [180.28767]\n",
      " [168.17859]\n",
      " [160.86717]\n",
      " [220.47185]\n",
      " [182.20634]\n",
      " [177.52809]\n",
      " [215.53915]\n",
      " [169.63698]\n",
      " [216.79878]\n",
      " [211.95097]\n",
      " [191.02477]\n",
      " [207.31581]\n",
      " [199.10211]\n",
      " [198.58775]\n",
      " [185.76683]\n",
      " [220.41205]]\n",
      "100 Cost:  29.96067 \n",
      "Prediction:\n",
      " [[159.19716 ]\n",
      " [180.09506 ]\n",
      " [183.14941 ]\n",
      " [199.74309 ]\n",
      " [134.4879  ]\n",
      " [ 98.592316]\n",
      " [153.33655 ]\n",
      " [119.274864]\n",
      " [166.21332 ]\n",
      " [153.78705 ]\n",
      " [144.09499 ]\n",
      " [137.33032 ]\n",
      " [188.63159 ]\n",
      " [155.67926 ]\n",
      " [152.41959 ]\n",
      " [184.10239 ]\n",
      " [144.4308  ]\n",
      " [186.66086 ]\n",
      " [181.52097 ]\n",
      " [163.76239 ]\n",
      " [177.96426 ]\n",
      " [170.05411 ]\n",
      " [170.55014 ]\n",
      " [159.27858 ]\n",
      " [188.19113 ]]\n",
      "200 Cost:  28.266949 \n",
      "Prediction:\n",
      " [[158.97284 ]\n",
      " [180.23303 ]\n",
      " [183.07059 ]\n",
      " [199.71515 ]\n",
      " [134.64658 ]\n",
      " [ 98.846176]\n",
      " [153.28726 ]\n",
      " [119.18666 ]\n",
      " [166.5075  ]\n",
      " [154.22389 ]\n",
      " [144.10956 ]\n",
      " [137.54672 ]\n",
      " [188.49826 ]\n",
      " [155.51328 ]\n",
      " [152.43738 ]\n",
      " [184.25986 ]\n",
      " [144.329   ]\n",
      " [186.54382 ]\n",
      " [181.3219  ]\n",
      " [163.55063 ]\n",
      " [177.94444 ]\n",
      " [170.21387 ]\n",
      " [170.4802  ]\n",
      " [158.92917 ]\n",
      " [188.24301 ]]\n",
      "300 Cost:  26.70155 \n",
      "Prediction:\n",
      " [[158.75716 ]\n",
      " [180.36635 ]\n",
      " [182.99516 ]\n",
      " [199.68803 ]\n",
      " [134.80038 ]\n",
      " [ 99.09018 ]\n",
      " [153.2385  ]\n",
      " [119.099106]\n",
      " [166.79027 ]\n",
      " [154.64238 ]\n",
      " [144.12311 ]\n",
      " [137.75433 ]\n",
      " [188.3712  ]\n",
      " [155.35565 ]\n",
      " [152.45308 ]\n",
      " [184.41121 ]\n",
      " [144.234   ]\n",
      " [186.42903 ]\n",
      " [181.13156 ]\n",
      " [163.3478  ]\n",
      " [177.92407 ]\n",
      " [170.36736 ]\n",
      " [170.41185 ]\n",
      " [158.5949  ]\n",
      " [188.2938  ]]\n",
      "400 Cost:  25.254236 \n",
      "Prediction:\n",
      " [[158.54977]\n",
      " [180.49515]\n",
      " [182.92299]\n",
      " [199.66165]\n",
      " [134.94946]\n",
      " [ 99.3247 ]\n",
      " [153.19019]\n",
      " [119.01224]\n",
      " [167.06207]\n",
      " [155.04327]\n",
      " [144.13565]\n",
      " [137.95352]\n",
      " [188.25005]\n",
      " [155.20593]\n",
      " [152.4668 ]\n",
      " [184.55667]\n",
      " [144.14545]\n",
      " [186.3164 ]\n",
      " [180.94955]\n",
      " [163.15344]\n",
      " [177.90317]\n",
      " [170.51486]\n",
      " [170.34502]\n",
      " [158.27509]\n",
      " [188.34349]]\n",
      "500 Cost:  23.91576 \n",
      "Prediction:\n",
      " [[158.35036]\n",
      " [180.61963]\n",
      " [182.85394]\n",
      " [199.63603]\n",
      " [135.09395]\n",
      " [ 99.55015]\n",
      " [153.14241]\n",
      " [118.92611]\n",
      " [167.3234 ]\n",
      " [155.42735]\n",
      " [144.14731]\n",
      " [138.14467]\n",
      " [188.1346 ]\n",
      " [155.06381]\n",
      " [152.47868]\n",
      " [184.69652]\n",
      " [144.063  ]\n",
      " [186.20598]\n",
      " [180.77553]\n",
      " [162.96727]\n",
      " [177.88184]\n",
      " [170.65659]\n",
      " [170.2797 ]\n",
      " [157.9691 ]\n",
      " [188.39214]]\n",
      "600 Cost:  22.677471 \n",
      "Prediction:\n",
      " [[158.15857 ]\n",
      " [180.73993 ]\n",
      " [182.78784 ]\n",
      " [199.61111 ]\n",
      " [135.23404 ]\n",
      " [ 99.76688 ]\n",
      " [153.09517 ]\n",
      " [118.840744]\n",
      " [167.57462 ]\n",
      " [155.79532 ]\n",
      " [144.15807 ]\n",
      " [138.32806 ]\n",
      " [188.02455 ]\n",
      " [154.92888 ]\n",
      " [152.4888  ]\n",
      " [184.83095 ]\n",
      " [143.98634 ]\n",
      " [186.09766 ]\n",
      " [180.60913 ]\n",
      " [162.78891 ]\n",
      " [177.86009 ]\n",
      " [170.7928  ]\n",
      " [170.21585 ]\n",
      " [157.67635 ]\n",
      " [188.43973 ]]\n",
      "700 Cost:  21.531599 \n",
      "Prediction:\n",
      " [[157.97412]\n",
      " [180.85622]\n",
      " [182.72461]\n",
      " [199.58688]\n",
      " [135.36984]\n",
      " [ 99.97525]\n",
      " [153.04846]\n",
      " [118.75621]\n",
      " [167.81616]\n",
      " [156.14786]\n",
      " [144.168  ]\n",
      " [138.50407]\n",
      " [187.91968]\n",
      " [154.80084]\n",
      " [152.49728]\n",
      " [184.96017]\n",
      " [143.91516]\n",
      " [185.99146]\n",
      " [180.45001]\n",
      " [162.61801]\n",
      " [177.83801]\n",
      " [170.9237 ]\n",
      " [170.15343]\n",
      " [157.39626]\n",
      " [188.4863 ]]\n",
      "800 Cost:  20.470911 \n",
      "Prediction:\n",
      " [[157.79672]\n",
      " [180.96863]\n",
      " [182.66412]\n",
      " [199.56335]\n",
      " [135.50153]\n",
      " [100.1756 ]\n",
      " [153.00232]\n",
      " [118.67254]\n",
      " [168.04843]\n",
      " [156.48561]\n",
      " [144.17715]\n",
      " [138.67297]\n",
      " [187.81978]\n",
      " [154.67935]\n",
      " [152.50426]\n",
      " [185.08447]\n",
      " [143.84918]\n",
      " [185.88736]\n",
      " [180.2979 ]\n",
      " [162.45432]\n",
      " [177.81564]\n",
      " [171.04955]\n",
      " [170.09245]\n",
      " [157.12831]\n",
      " [188.53188]]\n",
      "900 Cost:  19.48872 \n",
      "Prediction:\n",
      " [[157.62607 ]\n",
      " [181.0773  ]\n",
      " [182.60623 ]\n",
      " [199.54047 ]\n",
      " [135.6292  ]\n",
      " [100.36823 ]\n",
      " [152.95676 ]\n",
      " [118.589745]\n",
      " [168.27177 ]\n",
      " [156.80922 ]\n",
      " [144.18555 ]\n",
      " [138.83508 ]\n",
      " [187.72458 ]\n",
      " [154.56409 ]\n",
      " [152.50978 ]\n",
      " [185.20395 ]\n",
      " [143.78812 ]\n",
      " [185.78531 ]\n",
      " [180.15244 ]\n",
      " [162.29744 ]\n",
      " [177.793   ]\n",
      " [171.17049 ]\n",
      " [170.03284 ]\n",
      " [156.87192 ]\n",
      " [188.57645 ]]\n",
      "1000 Cost:  18.578968 \n",
      "Prediction:\n",
      " [[157.4619 ]\n",
      " [181.18236]\n",
      " [182.55083]\n",
      " [199.5182 ]\n",
      " [135.75299]\n",
      " [100.55348]\n",
      " [152.91176]\n",
      " [118.50787]\n",
      " [168.48656]\n",
      " [157.11928]\n",
      " [144.19325]\n",
      " [138.99065]\n",
      " [187.63387]\n",
      " [154.45479]\n",
      " [152.51396]\n",
      " [185.31883]\n",
      " [143.73169]\n",
      " [185.68529]\n",
      " [180.01335]\n",
      " [162.14714]\n",
      " [177.77016]\n",
      " [171.28676]\n",
      " [169.97458]\n",
      " [156.62663]\n",
      " [188.62007]]\n",
      "1100 Cost:  17.736032 \n",
      "Prediction:\n",
      " [[157.304  ]\n",
      " [181.28395]\n",
      " [182.49783]\n",
      " [199.49658]\n",
      " [135.87306]\n",
      " [100.73162]\n",
      " [152.86739]\n",
      " [118.42694]\n",
      " [168.69313]\n",
      " [157.41637]\n",
      " [144.2003 ]\n",
      " [139.13998]\n",
      " [187.54747]\n",
      " [154.35115]\n",
      " [152.51688]\n",
      " [185.42934]\n",
      " [143.67967]\n",
      " [185.58727]\n",
      " [179.88039]\n",
      " [162.00314]\n",
      " [177.74715]\n",
      " [171.39854]\n",
      " [169.91768]\n",
      " [156.39198]\n",
      " [188.66273]]\n",
      "1200 Cost:  16.954693 \n",
      "Prediction:\n",
      " [[157.15205]\n",
      " [181.38217]\n",
      " [182.44711]\n",
      " [199.47556]\n",
      " [135.98953]\n",
      " [100.90297]\n",
      " [152.8236 ]\n",
      " [118.34699]\n",
      " [168.89182]\n",
      " [157.70105]\n",
      " [144.20673]\n",
      " [139.28333]\n",
      " [187.4652 ]\n",
      " [154.2529 ]\n",
      " [152.51862]\n",
      " [185.53563]\n",
      " [143.6318 ]\n",
      " [185.49123]\n",
      " [179.75325]\n",
      " [161.86516]\n",
      " [177.724  ]\n",
      " [171.50601]\n",
      " [169.86206]\n",
      " [156.16745]\n",
      " [188.70447]]\n",
      "1300 Cost:  16.230228 \n",
      "Prediction:\n",
      " [[157.00584]\n",
      " [181.47719]\n",
      " [182.39856]\n",
      " [199.45511]\n",
      " [136.10248]\n",
      " [101.06777]\n",
      " [152.78041]\n",
      " [118.26802]\n",
      " [169.08295]\n",
      " [157.97383]\n",
      " [144.21254]\n",
      " [139.42091]\n",
      " [187.38681]\n",
      " [154.1598 ]\n",
      " [152.51924]\n",
      " [185.63785]\n",
      " [143.58788]\n",
      " [185.39713]\n",
      " [179.6317 ]\n",
      " [161.73293]\n",
      " [177.70073]\n",
      " [171.60933]\n",
      " [169.80771]\n",
      " [155.95264]\n",
      " [188.74529]]\n",
      "1400 Cost:  15.558274 \n",
      "Prediction:\n",
      " [[156.86516 ]\n",
      " [181.56908 ]\n",
      " [182.35211 ]\n",
      " [199.43521 ]\n",
      " [136.21204 ]\n",
      " [101.22628 ]\n",
      " [152.73782 ]\n",
      " [118.190056]\n",
      " [169.2668  ]\n",
      " [158.23523 ]\n",
      " [144.21782 ]\n",
      " [139.55301 ]\n",
      " [187.31218 ]\n",
      " [154.0716  ]\n",
      " [152.51884 ]\n",
      " [185.73616 ]\n",
      " [143.54765 ]\n",
      " [185.30493 ]\n",
      " [179.51547 ]\n",
      " [161.60623 ]\n",
      " [177.6774  ]\n",
      " [171.70871 ]\n",
      " [169.75461 ]\n",
      " [155.7471  ]\n",
      " [188.78522 ]]\n",
      "1500 Cost:  14.934803 \n",
      "Prediction:\n",
      " [[156.72977]\n",
      " [181.65796]\n",
      " [182.30765]\n",
      " [199.41588]\n",
      " [136.31831]\n",
      " [101.37879]\n",
      " [152.69586]\n",
      " [118.11312]\n",
      " [169.44368]\n",
      " [158.48572]\n",
      " [144.22255]\n",
      " [139.67981]\n",
      " [187.24109]\n",
      " [153.98804]\n",
      " [152.51747]\n",
      " [185.83075]\n",
      " [143.51094]\n",
      " [185.21461]\n",
      " [179.40434]\n",
      " [161.4848 ]\n",
      " [177.65399]\n",
      " [171.80424]\n",
      " [169.7027 ]\n",
      " [155.55045]\n",
      " [188.82425]]\n",
      "1600 Cost:  14.356115 \n",
      "Prediction:\n",
      " [[156.59946]\n",
      " [181.74393]\n",
      " [182.26512]\n",
      " [199.39708]\n",
      " [136.42143]\n",
      " [101.52549]\n",
      " [152.65453]\n",
      " [118.03721]\n",
      " [169.61386]\n",
      " [158.72578]\n",
      " [144.2268 ]\n",
      " [139.80156]\n",
      " [187.17343]\n",
      " [153.90892]\n",
      " [152.51523]\n",
      " [185.92175]\n",
      " [143.47754]\n",
      " [185.12617]\n",
      " [179.2981 ]\n",
      " [161.36844]\n",
      " [177.63062]\n",
      " [171.89615]\n",
      " [169.65202]\n",
      " [155.3623 ]\n",
      " [188.86243]]\n",
      "1700 Cost:  13.8187895 \n",
      "Prediction:\n",
      " [[156.47403]\n",
      " [181.8271 ]\n",
      " [182.22444]\n",
      " [199.3788 ]\n",
      " [136.52148]\n",
      " [101.66664]\n",
      " [152.61382]\n",
      " [117.96236]\n",
      " [169.77762]\n",
      " [158.95586]\n",
      " [144.23059]\n",
      " [139.91846]\n",
      " [187.109  ]\n",
      " [153.83403]\n",
      " [152.51213]\n",
      " [186.00931]\n",
      " [143.44727]\n",
      " [185.03954]\n",
      " [179.19652]\n",
      " [161.25693]\n",
      " [177.60724]\n",
      " [171.98453]\n",
      " [169.6025 ]\n",
      " [155.18228]\n",
      " [188.89977]]\n",
      "1800 Cost:  13.319736 \n",
      "Prediction:\n",
      " [[156.35329]\n",
      " [181.90758]\n",
      " [182.18547]\n",
      " [199.36101]\n",
      " [136.61855]\n",
      " [101.80247]\n",
      " [152.57372]\n",
      " [117.88856]\n",
      " [169.93518]\n",
      " [159.17633]\n",
      " [144.23393]\n",
      " [140.03069]\n",
      " [187.04767]\n",
      " [153.76317]\n",
      " [152.50827]\n",
      " [186.09355]\n",
      " [143.41994]\n",
      " [184.9547 ]\n",
      " [179.09941]\n",
      " [161.15004]\n",
      " [177.58388]\n",
      " [172.06953]\n",
      " [169.5541 ]\n",
      " [155.01006]\n",
      " [188.93626]]\n",
      "1900 Cost:  12.856009 \n",
      "Prediction:\n",
      " [[156.23709]\n",
      " [181.98547]\n",
      " [182.14821]\n",
      " [199.34372]\n",
      " [136.71277]\n",
      " [101.93316]\n",
      " [152.53427]\n",
      " [117.81582]\n",
      " [170.08684]\n",
      " [159.38766]\n",
      " [144.23686]\n",
      " [140.13849]\n",
      " [186.98932]\n",
      " [153.69614]\n",
      " [152.5037 ]\n",
      " [186.17462]\n",
      " [143.3954 ]\n",
      " [184.87164]\n",
      " [179.00658]\n",
      " [161.04764]\n",
      " [177.56061]\n",
      " [172.1513 ]\n",
      " [169.50684]\n",
      " [154.84526]\n",
      " [188.97195]]\n",
      "2000 Cost:  12.424991 \n",
      "Prediction:\n",
      " [[156.1252  ]\n",
      " [182.06084 ]\n",
      " [182.11255 ]\n",
      " [199.3269  ]\n",
      " [136.80417 ]\n",
      " [102.058945]\n",
      " [152.49544 ]\n",
      " [117.744156]\n",
      " [170.23279 ]\n",
      " [159.5902  ]\n",
      " [144.23941 ]\n",
      " [140.24199 ]\n",
      " [186.93379 ]\n",
      " [153.63275 ]\n",
      " [152.49846 ]\n",
      " [186.25266 ]\n",
      " [143.37347 ]\n",
      " [184.79031 ]\n",
      " [178.91782 ]\n",
      " [160.94946 ]\n",
      " [177.53741 ]\n",
      " [172.22998 ]\n",
      " [169.46066 ]\n",
      " [154.68759 ]\n",
      " [189.00687 ]]\n"
     ]
    }
   ],
   "source": [
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score will be  [[172.14961]]\n",
      "Other scores will be  [[184.15213]\n",
      " [180.41133]]\n"
     ]
    }
   ],
   "source": [
    "# Ask my score - prediction\n",
    "print(\"Your score will be \", sess.run(\n",
    "    hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \", sess.run(hypothesis,\n",
    "                                        feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy에서 파일 전체를 올리기에 메모리가 부족하다면\n",
    "# tensorflow에서는 Queue Runners 지원\n",
    "# 데이타의 양이 많기 때문에 메모리에 모두 적재하고 학습을 할 수 가 없고, 파일에서 읽어드리면서 학습을 해야 한다.\n",
    "# 파일에서 데이타를 읽어가면서, 읽은 데이타를 순차적으로 모델에 피딩하면 되는데, 이때 큐를 사용한다.\n",
    "# 파일에서 데이타를 읽는 방법에 앞서서 큐를 설명하면, 큐에 데이타를 넣는 것(Enqueue) 은 Queue Runner 라는 것이 한다. \n",
    "# Queue Runner가 큐에 어떤 데이타를 어떻게 넣을지를 정의 하는 것이 Enqueue_operation인데, 데이타를 읽어서 실제로 어떻게 Queue에 Enqueue 하는지를 정의한다.\n",
    "# Queue Runner는 멀티 쓰레드로 작동하는데, Queue Runner 안의 쓰레드들을 관리해주기 위해서 별도로 Coordinator라는 것을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#파일 리스트\n",
    "# filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\"] - txt파일 input pipeline\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "filenames = ['data-01-test-score.csv']\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]   #  default일 경우\n",
    "dataset = tf.contrib.data.CsvDataset(filenames, record_defaults)\n",
    "\n",
    "dataset = dataset.batch(10)\n",
    "\n",
    "# collect batches of csv in - xy데이터를 한 번에 10번씩 가져온다\n",
    "# 펌프로 데이터를 빨아들이듯이 한 번 펌프에 10개씩 데이터를 가져옴\n",
    "# 아래는 tensor 그래프\n",
    "# train_x_batch, train_y_batch = dataset[0:-1], dataset[-1:]\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Python\\lib\\site-packages\\tensorflow\\python\\training\\input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-13-bc0524649781>:46: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#파일 리스트\n",
    "# filename_queue = tf.train.string_input_producer(\n",
    "#     ['data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "filenames = ['data-01-test-score.csv']\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "# reader 정의하는 부분\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "record_defaults_ex = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults_ex)\n",
    "\n",
    "# collect batches of csv in - xy데이터를 한 번에 10번씩 가져온다\n",
    "# 펌프로 데이터를 빨아들이듯이 한 번 펌프에 10개씩 데이터를 가져옴\n",
    "# 아래는 tensor 그래프\n",
    "train_x_batch, train_y_batch = \\\n",
    "    tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess: # with 함수를 쓰면 open 하는 것들을 코드가 끝나고 자동으로 닫아줌.\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Start populating the filename queue.\n",
    "    # queue 에서 통상적으로 사용\n",
    "    # queue runner는 멀티 쓰레드로 작동, 멀티 쓰레드로 작동하기 위해서는 별도의 coord가 필요\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    for step in range(2001):\n",
    "        # 위에는 batch tensor만 만들어 놓았고\n",
    "        # 10개씩 펌프로 데이터 가져오는 tensor 실행\n",
    "        x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "        cost_val, hy_val, _ = sess.run(\n",
    "            [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n",
    "        if step % 100 == 0:\n",
    "            print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "    # queue 에서 통상적으로 사용\n",
    "    # 큐 사용이 끝났으면 Queue Runner의 쓰레드들을 모두 정지 시켜야 한다\n",
    "    coord.request_stop()\n",
    "    # 다음 코드를 진행하기전에, Queue Runner의 모든 쓰레드들이 정지될때 까지 기다리는 코드\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask my score\n",
    "print(\"Your score will be \",\n",
    "      sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))\n",
    "\n",
    "print(\"Other scores will be \",\n",
    "      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
